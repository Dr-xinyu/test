{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# cnn ç¥ç»ç½‘ç»œå®ç°\n",
    "ä¸ä½¿ç”¨pytorch å®ç°cnn\n",
    "éœ€è¦åœ¨ backward çš„åŸºç¡€ä¹‹ä¸Š åŠ ä¸Š å·ç§¯å±‚ã€æ± åŒ–å±‚çš„å®ç°å°±å¥½"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88695090c2ced300"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## å·ç§¯å±‚\n",
    "å·ç§¯å±‚æœ‰ä¸¤ä¸ªä¸œè¥¿ï¼Œä¸€ä¸ªæ˜¯data ä¸€ä¸ªæ˜¯filter(kernel),æ•´ä¸ªæµç¨‹å¦‚ä¸‹å›¾æ‰€ç¤º\n",
    "<img src=\"å·ç§¯è¿ç®—.jpg\" height=960>\n",
    "\n",
    "### å¡«å……padding\n",
    "ä½¿ç”¨å¡«å……ä¸»è¦æ˜¯ä¸ºäº†è°ƒæ•´è¾“å‡ºçš„å¤§å°ã€‚æ¯”å¦‚,å¯¹å¤§å°ä¸º (4, 4) çš„è¾“å…¥  æ•°æ®åº”ç”¨ (3, 3) çš„æ»¤æ³¢å™¨æ—¶,è¾“å‡ºå¤§å°å˜ä¸º (2, 2),ç›¸å½“äºè¾“å‡ºå¤§å°  æ¯”è¾“å…¥å¤§å°ç¼©å°äº† 2 ä¸ªå…ƒç´ ã€‚è¿™åœ¨åå¤è¿›è¡Œå¤šæ¬¡å·ç§¯è¿ç®—çš„æ·±åº¦ç½‘  ç»œä¸­ä¼šæˆä¸ºé—®é¢˜ã€‚ä¸ºä»€ä¹ˆå‘¢?å› ä¸ºå¦‚æœæ¯æ¬¡è¿›è¡Œå·ç§¯è¿ç®—éƒ½ä¼šç¼©å°  ç©ºé—´,é‚£ä¹ˆåœ¨æŸä¸ªæ—¶åˆ»è¾“å‡ºå¤§å°å°±æœ‰å¯èƒ½å˜ä¸º 1,å¯¼è‡´æ— æ³•å†åº”ç”¨  å·ç§¯è¿ç®—ã€‚ä¸ºäº†é¿å…å‡ºç°è¿™æ ·çš„æƒ…å†µ,å°±è¦ä½¿ç”¨å¡«å……ã€‚åœ¨åˆšæ‰çš„ä¾‹  å­ä¸­,å°†å¡«å……çš„å¹…åº¦è®¾ä¸º 1,é‚£ä¹ˆç›¸å¯¹äºè¾“å…¥å¤§å° (4, 4),è¾“å‡ºå¤§å°  ä¹Ÿä¿æŒä¸ºåŸæ¥çš„ (4, 4)ã€‚å› æ­¤,å·ç§¯è¿ç®—å°±å¯ä»¥åœ¨ä¿æŒç©ºé—´å¤§å°ä¸å˜  çš„æƒ…å†µä¸‹å°†æ•°æ®ä¼ ç»™ä¸‹ä¸€å±‚ã€‚\n",
    "\n",
    "### channel\n",
    "å½“å›¾ç‰‡ä¸æ˜¯ç°è‰²ï¼Œè€Œæ˜¯rgb 3é€šé“çš„æ•°æ®æ—¶ï¼Œfilter ä¹Ÿåº”è¯¥æ˜¯3é€šé“çš„ï¼Œè¿™æ ·å·ç§¯å‡ºæ¥çš„ä¼šåç¼©æˆäºŒç»´æ•°æ®,å¦‚å›¾æ‰€ç¤ºã€‚\n",
    "<br>\n",
    "<img src=\"n_channel.jpg\">\n",
    "<br>\n",
    "å¦‚æœä½ æƒ³å·ç§¯åçš„é€šé“æ•°å’Œå·ç§¯å‰çš„ä¸€æ ·ï¼Œé‚£ä¹ˆåº”è¯¥ä½¿ç”¨å¤šä¸ªfilterï¼Œå¹¶ä¸”æ¯ä¸ªfilteréƒ½æ˜¯å’Œå›¾ç‰‡ç›¸åŒçš„é€šé“æ•°ï¼Œå¦‚å›¾æ‰€ç¤º\n",
    "<br>\n",
    "<img src=\"n_filter.jpg\">"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "db7d837e95deadd8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## æ± åŒ–å±‚\n",
    "æ± åŒ–æ˜¯ç¼©å°é«˜ã€é•¿æ–¹å‘ä¸Šçš„ç©ºé—´çš„è¿ç®—\n",
    "<br>\n",
    "<img src=\"max_pool.jpg\">\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7005236327cc1add"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## im2col and col2im\n",
    "å·ç§¯å±‚å’Œæ± åŒ–å±‚çš„å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­éœ€è¦ç”¨åˆ°çš„ä¸¤ä¸ªæ–¹æ³•ï¼Œç”¨äºç®€åŒ–è®¡ç®—\n",
    "åŸç†ï¼šim2col æ˜¯æŠŠ 3ç»´å›¾åƒæŒ‰ç…§æ¯ä¸ªå·ç§¯æ ¸çš„å¤§å°ï¼Œæ¯ä¸€ä¸ªéƒ½å±•å¼€æˆä¸€ä¸ªcol,å†æŠŠæ¯ä¸ª3é€šé“çš„filter å±•å¼€æˆä¸€åˆ—ã€‚ç„¶åå·ç§¯å°±æˆä¸ºäº†çŸ©é˜µä¹˜æ³•\n",
    "ä¸€ä¸ª3é€šé“å›¾åƒä¼šå˜æˆä¸€ä¸ªäºŒç»´çŸ©é˜µï¼Œæ¯ä¸€è¡Œéƒ½æ˜¯éœ€è¦è¿›è¡Œå·ç§¯è¿ç®—çš„å€¼ï¼Œç„¶å3é€šé“çš„filterä¹Ÿå˜æˆäº†ä¸€ä¸ªäºŒç»´çŸ©é˜µï¼Œæ¯ä¸€åˆ—éƒ½æ˜¯ä¸€ä¸ªfilterã€‚è¿™ä¸¤ä¸ªçŸ©é˜µè¿›è¡ŒçŸ©é˜µä¹˜æ³•ï¼Œç„¶åéœ€è¦æŠŠå¾—åˆ°çš„æ–°çŸ©é˜µåšreshape->æ–°çŸ©é˜µçš„æ¯ä¸€åˆ—éƒ½æ˜¯ä¸€æ¬¡å·ç§¯çš„ç»“æœï¼Œéƒ½åº”è¯¥reshapeæˆä¸€ä¸ªOHxOWçš„çŸ©é˜µï¼Œç„¶åå¦‚æœæ˜¯æœ‰fnä¸ªfilterçš„ï¼Œæ–°çš„çŸ©é˜µåº”è¯¥ä¼šæœ‰fnåˆ—ï¼Œå°±è¦reshapeæˆfnxOHxOW\n",
    "<br>\n",
    "<img src=\"img/im2col.jpg\">\n",
    "<br>\n",
    "\n",
    "<img src=\"img/im2col_all.jpg\">"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a373fd4395a9ce2d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "import os, time, random\n",
    "from typing import List, Tuple\n",
    "from PIL import Image, ImageOps,ImageEnhance, ImageChops, ImageFilter\n",
    "from tqdm import tqdm\n",
    "import json"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-20T07:36:24.783313Z",
     "start_time": "2025-11-20T07:36:24.781355Z"
    }
   },
   "id": "6e1abe275ed02cab",
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def softmax(z: np.ndarray) -> np.ndarray:\n",
    "    z = z - z.max(axis=1, keepdims=True)\n",
    "    ez = np.exp(z, dtype=np.float32)\n",
    "    return ez / (np.sum(ez, axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "def cross_entropy_loss(probs: np.ndarray, y_true_idx: np.ndarray):\n",
    "    \"\"\"\n",
    "    probs: (N, C) after softmax\n",
    "    y_true_idx: (N,) integer labels\n",
    "    returns (loss, grad_logits)\n",
    "    \"\"\"\n",
    "    N = probs.shape[0]\n",
    "    logp = -np.log(probs[np.arange(N), y_true_idx] + 1e-12)\n",
    "    loss = float(np.mean(logp))\n",
    "    return loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-20T07:36:24.863636Z",
     "start_time": "2025-11-20T07:36:24.861368Z"
    }
   },
   "id": "73fc95f46df149b2",
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Adam:\n",
    "\n",
    "    \"\"\"Adam (http://arxiv.org/abs/1412.6980v8)\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m, self.v = {}, {}\n",
    "            for key, val in params.items():\n",
    "                self.m[key] = np.zeros_like(val)\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "        \n",
    "        self.iter += 1\n",
    "        lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)         \n",
    "        \n",
    "        for key in params.keys():\n",
    "            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n",
    "            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\n",
    "            \n",
    "            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-20T07:36:24.870549Z",
     "start_time": "2025-11-20T07:36:24.864292Z"
    }
   },
   "id": "972a342732a5f591",
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def augment_pil(img, img_size):\n",
    "    # éšæœºæ°´å¹³/å‚ç›´ç¿»è½¬\n",
    "    if random.random() < 0.5:\n",
    "        img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "    if random.random() < 0.3:\n",
    "        img = img.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "\n",
    "    # éšæœºæ—‹è½¬ Â±15Â°\n",
    "    if random.random() < 0.7:\n",
    "        angle = random.uniform(-15, 15)\n",
    "        img = img.rotate(angle, resample=Image.BILINEAR, fillcolor=0)\n",
    "\n",
    "    # éšæœºå¹³ç§»ï¼ˆæ¨¡æ‹Ÿä½ç½®åç§»ï¼‰\n",
    "    if random.random() < 0.6:\n",
    "        w, h = img.size\n",
    "        max_dx = int(0.1 * w)\n",
    "        max_dy = int(0.1 * h)\n",
    "        dx = random.randint(-max_dx, max_dx)\n",
    "        dy = random.randint(-max_dy, max_dy)\n",
    "        img = ImageChops.offset(img, dx, dy)\n",
    "        img = img.crop((max_dx, max_dy, w-max_dx, h-max_dy))  # crop back\n",
    "        img = img.resize((img_size, img_size), Image.BILINEAR)\n",
    "\n",
    "    # éšæœºäº®åº¦/å¯¹æ¯”åº¦\n",
    "    if random.random() < 0.6:\n",
    "        factor = random.uniform(0.7, 1.4)\n",
    "        img = ImageEnhance.Brightness(img).enhance(factor)\n",
    "    if random.random() < 0.6:\n",
    "        factor = random.uniform(0.7, 1.4)\n",
    "        img = ImageEnhance.Contrast(img).enhance(factor)\n",
    "\n",
    "    # è½»å¾®é«˜æ–¯æ¨¡ç³Šï¼ˆé˜²é”åŒ–è¿‡æ‹Ÿåˆï¼‰\n",
    "    if random.random() < 0.2:\n",
    "        img = img.filter(ImageFilter.GaussianBlur(radius=random.uniform(0.3, 0.8)))\n",
    "\n",
    "    # éšæœºæ“¦é™¤ï¼ˆCoarseDropout æ¨¡æ‹Ÿï¼‰\n",
    "    if random.random() < 0.3:\n",
    "        arr = np.array(img)\n",
    "        h, w = arr.shape\n",
    "        for _ in range(random.randint(1, 6)):\n",
    "            x = random.randint(0, w-4)\n",
    "            y = random.randint(0, h-4)\n",
    "            arr[y:y+random.randint(1,4), x:x+random.randint(1,4)] = 0\n",
    "        img = Image.fromarray(arr)\n",
    "\n",
    "    return img"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-20T07:36:24.876660Z",
     "start_time": "2025-11-20T07:36:24.870912Z"
    }
   },
   "id": "d1546991b1e37cb6",
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "def im2col(input_data, kernel_h, kernel_w, stride=1, pad=0):\n",
    "    \"\"\"\n",
    "    å°†è¾“å…¥è½¬æ¢ä¸ºåˆ—çŸ©é˜µï¼Œæ¯ä¸ª filter å¤§å°çš„å—å±•å¹³ä¸ºä¸€è¡Œã€‚\n",
    "    \n",
    "    input_data: shape (N, C, H, W)\n",
    "    è¿”å›: shape (N * out_h * out_w, C * kernel_h * kernel_w)\n",
    "    \"\"\"\n",
    "    N, C, H, W = input_data.shape\n",
    "\n",
    "    out_h = (H + 2 * pad - kernel_h) // stride + 1\n",
    "    out_w = (W + 2 * pad - kernel_w) // stride + 1\n",
    "\n",
    "    # å¡«å……\n",
    "    img = np.pad(input_data, ((0, 0), (0, 0), (pad, pad), (pad, pad)), mode='constant')\n",
    "\n",
    "    # åˆå§‹åŒ– col çŸ©é˜µ\n",
    "    col = np.zeros((N, C, kernel_h, kernel_w, out_h, out_w))\n",
    "\n",
    "    # åˆ©ç”¨æ­¥è¿›æå–æ¯ä¸ªçª—å£\n",
    "    # è¿™ä¸€æ­¥å¾ˆæœ‰è¶£ï¼Œè™½ç„¶è¯´æ˜¯æ¯ä¸€æ¬¡æŠŠfilterå¯¹åº”çš„img data é“ºå¹³æˆä¸€è¡Œï¼Œä½†æ˜¯è¿™è¾¹ä¸æ˜¯è¿™æ ·åšçš„\n",
    "    for i in range(kernel_h):\n",
    "        i_max = i + stride * out_h\n",
    "        for j in range(kernel_w):\n",
    "            j_max = j + stride * out_w\n",
    "            col[:, :, i, j, :, :] = img[:, :, i:i_max:stride, j:j_max:stride]\n",
    "        \n",
    "\n",
    "    # è½¬ç½® + reshapeï¼šæŠŠ (N, out_h, out_w, C, K_h, K_w) -> (N*out_h*out_w, C*K_h*K_w)\n",
    "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N * out_h * out_w, -1)\n",
    "    return col\n",
    "\n",
    "def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    col :\n",
    "    input_shape : è¾“å…¥æ•°æ®çš„å½¢çŠ¶ï¼ˆä¾‹ï¼š(10, 1, 28, 28)ï¼‰\n",
    "    filter_h :\n",
    "    filter_w\n",
    "    stride\n",
    "    pad\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    \"\"\"\n",
    "    N, C, H, W = input_shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
    "\n",
    "    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
    "\n",
    "    return img[:, :, pad:H + pad, pad:W + pad]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-20T07:36:24.884890Z",
     "start_time": "2025-11-20T07:36:24.881677Z"
    }
   },
   "id": "ad50e73ec2a61f26",
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Convolution:\n",
    "    def __init__(self, K, b, stride = 1, pad = 0):\n",
    "        self.K = K\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        # ä¸­é—´æ•°æ®ï¼ˆbackwardæ—¶ä½¿ç”¨ï¼‰\n",
    "        self.x = None   \n",
    "        self.col = None\n",
    "        self.col_K = None\n",
    "        \n",
    "        # æƒé‡å’Œåç½®å‚æ•°çš„æ¢¯åº¦\n",
    "        self.dK = None\n",
    "        self.db = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        fn, c, kh, kw = self.K.shape\n",
    "        N,C,H,W = x.shape\n",
    "        out_h = (H + 2* self.pad -kh)//self.stride + 1\n",
    "        out_w = (W + 2* self.pad -kw)//self.stride + 1\n",
    "        col = im2col(x,kh, kw, self.stride, self.pad)\n",
    "        col_K = self.K.reshape(fn, -1).T\n",
    "        out = col@col_K + self.b\n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "        \n",
    "        self.x = x\n",
    "        self.col = col\n",
    "        self.col_K = col_K\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self,dout):\n",
    "        FN, C, FH, FW = self.K.shape\n",
    "        dout = dout.transpose(0,2,3,1).reshape(-1, FN)\n",
    "\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        self.dK = np.dot(self.col.T, dout)\n",
    "        self.dK = self.dK.transpose(1, 0).reshape(FN, C, FH, FW)\n",
    "\n",
    "        dcol = dout @ self.col_K.T\n",
    "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
    "\n",
    "        return dx\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-20T07:36:24.890929Z",
     "start_time": "2025-11-20T07:36:24.885340Z"
    }
   },
   "id": "cf0c727f13707610",
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        self.x = None\n",
    "        self.arg_max = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
    "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
    "\n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
    "\n",
    "        arg_max = np.argmax(col, axis=1)\n",
    "        out = np.max(col, axis=1)\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
    "\n",
    "        self.x = x\n",
    "        self.arg_max = arg_max\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout = dout.transpose(0, 2, 3, 1)\n",
    "        \n",
    "        pool_size = self.pool_h * self.pool_w\n",
    "        dmax = np.zeros((dout.size, pool_size))\n",
    "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
    "        dmax = dmax.reshape(dout.shape + (pool_size,)) \n",
    "        \n",
    "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
    "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        \n",
    "        return dx"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-20T07:36:24.896816Z",
     "start_time": "2025-11-20T07:36:24.891545Z"
    }
   },
   "id": "11457bf67747b870",
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class simpleConvNet:\n",
    "    # conv -> relu -> pool ->conv->relu->pool->full->relu->full\n",
    "        def __init__(self, input_dim=[1,28,28], \n",
    "                 conv_param={'filter_nums':[8, 16], 'filter_size':5, 'pad':0, 'stride':1},\n",
    "                 hidden_size=64, num_classes=5, weight_init_std=0.01):\n",
    "            filter_nums = conv_param['filter_nums']\n",
    "            filter_size = conv_param['filter_size']\n",
    "            filter_pad = conv_param['pad']\n",
    "            filter_stride = conv_param['stride']\n",
    "            input_size = input_dim[1]\n",
    "    \n",
    "            # åˆå§‹åŒ–æƒé‡\n",
    "            self.params = {}\n",
    "            self.params['K1'] = np.random.randn(filter_nums[0], input_dim[0], filter_size, filter_size).astype(np.float32) * np.sqrt(2.0 / (filter_size * filter_size * input_dim[0]))\n",
    "            \n",
    "            self.params['b1'] = np.zeros(filter_nums[0])\n",
    "            self.params['K2'] = np.random.randn(filter_nums[1], filter_nums[0], filter_size, filter_size).astype(np.float32) * np.sqrt(2.0 / (filter_size * filter_size * filter_nums[0]))\n",
    "            self.params['b2'] = np.zeros(filter_nums[1])\n",
    "            \n",
    "            s1 = input_size - filter_size + 1\n",
    "            s1p = s1 // 2\n",
    "            s2 = s1p - filter_size + 1\n",
    "            s2p = s2 // 2\n",
    "            self.flat_dim = s2p * s2p * filter_nums[1]\n",
    "            \n",
    "            self.params['W1'] = np.random.randn(self.flat_dim, hidden_size).astype(np.float32) * np.sqrt(2.0 / self.flat_dim)\n",
    "            self.params['b3'] = np.zeros((hidden_size,), dtype=np.float32)\n",
    "            self.params['W2'] = np.random.randn(hidden_size, num_classes).astype(np.float32) * np.sqrt(2.0 / hidden_size)\n",
    "            self.params['b4'] = np.zeros((num_classes,), dtype=np.float32)\n",
    "    \n",
    "            # ç”Ÿæˆå±‚\n",
    "            self.layers = OrderedDict()\n",
    "            self.layers['Conv1'] = Convolution(self.params['K1'], self.params['b1'],\n",
    "                                               filter_stride, filter_pad)\n",
    "            self.layers['Relu1'] = Relu()\n",
    "            self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "            self.layers['Conv2'] = Convolution(self.params['K2'], self.params['b2'], filter_stride, filter_pad)\n",
    "            self.layers['Relu2'] = Relu()\n",
    "            self.layers['Pool2'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "            self.layers['Affine1'] = Affine(self.params['W1'], self.params['b3'])\n",
    "            self.layers['Relu3'] = Relu()\n",
    "            self.layers['Affine2'] = Affine(self.params['W2'], self.params['b4'])\n",
    "    \n",
    "            self.last_layer = SoftmaxWithLoss()\n",
    "    \n",
    "        def predict(self, x):\n",
    "            for layer in self.layers.values():\n",
    "                x = layer.forward(x)\n",
    "    \n",
    "            return x\n",
    "    \n",
    "        def loss(self, x, t):\n",
    "            \"\"\"æ±‚æŸå¤±å‡½æ•°\n",
    "            \"\"\"\n",
    "            y = self.predict(x)\n",
    "            return y, self.last_layer.forward(y, t)\n",
    "    \n",
    "        def accuracy(self, x, t, batch_size=100):\n",
    "            if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "            \n",
    "            acc = 0.0\n",
    "            \n",
    "            for i in range(int(x.shape[0] / batch_size)):\n",
    "                tx = x[i*batch_size:(i+1)*batch_size]\n",
    "                tt = t[i*batch_size:(i+1)*batch_size]\n",
    "                y = self.predict(tx)\n",
    "                y = np.argmax(y, axis=1)\n",
    "                acc += np.sum(y == tt) \n",
    "            \n",
    "            return acc / x.shape[0]\n",
    "    \n",
    "        def gradient(self, x, t):\n",
    "            \"\"\"æ±‚æ¢¯åº¦ï¼ˆè¯¯å·®åå‘ä¼ æ’­æ³•ï¼‰\n",
    "    \n",
    "            Parameters\n",
    "            ----------\n",
    "            x : è¾“å…¥æ•°æ®\n",
    "            t : æ•™å¸ˆæ ‡ç­¾\n",
    "    \n",
    "            Returns\n",
    "            -------\n",
    "            å…·æœ‰å„å±‚çš„æ¢¯åº¦çš„å­—å…¸å˜é‡\n",
    "                grads['W1']ã€grads['W2']ã€...æ˜¯å„å±‚çš„æƒé‡\n",
    "                grads['b1']ã€grads['b2']ã€...æ˜¯å„å±‚çš„åç½®\n",
    "            \"\"\"\n",
    "    \n",
    "            # backward\n",
    "            dout = 1\n",
    "            dout = self.last_layer.backward(dout)\n",
    "    \n",
    "            layers = list(self.layers.values())\n",
    "            layers.reverse()\n",
    "            for layer in layers:\n",
    "                dout = layer.backward(dout)\n",
    "    \n",
    "            # è®¾å®š\n",
    "            grads = {}\n",
    "            grads['K1'], grads['b1'] = self.layers['Conv1'].dK, self.layers['Conv1'].db\n",
    "            grads['K2'], grads['b2'] = self.layers['Conv2'].dK, self.layers['Conv2'].db\n",
    "            grads['W1'], grads['b3'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "            grads['W2'], grads['b4'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "    \n",
    "            return grads\n",
    "            \n",
    "        def save_params(self, file_name=\"params.pkl\"):\n",
    "            params = {}\n",
    "            for key, val in self.params.items():\n",
    "                params[key] = val\n",
    "            with open(file_name, 'wb') as f:\n",
    "                pickle.dump(params, f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-20T07:36:24.922925Z",
     "start_time": "2025-11-20T07:36:24.897318Z"
    }
   },
   "id": "57d196651ae27e11",
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "\n",
    "        return dx\n",
    "\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W =W\n",
    "        self.b = b\n",
    "        \n",
    "        self.x = None\n",
    "        self.original_x_shape = None\n",
    "        # æƒé‡å’Œåç½®å‚æ•°çš„å¯¼æ•°\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # å¯¹åº”å¼ é‡\n",
    "        self.original_x_shape = x.shape\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        self.x = x\n",
    "\n",
    "        out = np.dot(self.x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        dx = dx.reshape(*self.original_x_shape)  # è¿˜åŸè¾“å…¥æ•°æ®çš„å½¢çŠ¶ï¼ˆå¯¹åº”å¼ é‡ï¼‰\n",
    "        return dx\n",
    "    \n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None # softmaxçš„è¾“å‡º\n",
    "        self.t = None # ç›‘ç£æ•°æ®\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss= cross_entropy_loss(self.y, self.t)\n",
    "        \n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        if self.t.size == self.y.size: # ç›‘ç£æ•°æ®æ˜¯one-hot-vectorçš„æƒ…å†µ\n",
    "            dx = (self.y - self.t) / batch_size\n",
    "        else:\n",
    "            dx = self.y.copy()\n",
    "            dx[np.arange(batch_size), self.t] -= 1\n",
    "            dx = dx / batch_size\n",
    "        \n",
    "        return dx"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-20T07:36:24.929271Z",
     "start_time": "2025-11-20T07:36:24.923696Z"
    }
   },
   "id": "39b77e4e1ffb39df",
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# ä¸€äº›å·¥å…·\n",
    "# === Simple dataset loader for folder structure ===\n",
    "IMG_SIZE = 100\n",
    "\n",
    "def accuracy(logits, y):\n",
    "    preds = np.argmax(softmax(logits), axis=1)\n",
    "    return float(np.mean(preds == y))\n",
    "\n",
    "def list_images(root: str) -> Tuple[list, list]:\n",
    "    classes = sorted([d for d in os.listdir(root) if os.path.isdir(os.path.join(root, d))])\n",
    "    items = []\n",
    "    for ci, cname in enumerate(classes):\n",
    "        cdir = os.path.join(root, cname)\n",
    "        for fn in os.listdir(cdir):\n",
    "            if fn.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp')):\n",
    "                items.append((os.path.join(cdir, fn), ci))\n",
    "    return items, classes\n",
    "\n",
    "def load_batch(paths_labels, start, end, img_size=IMG_SIZE, augment=False):\n",
    "    batch = paths_labels[start:end]\n",
    "    N = len(batch)\n",
    "    X = np.zeros((N, 1,img_size, img_size), dtype=np.float32)\n",
    "    y = np.zeros((N,), dtype=np.int64)\n",
    "    for i, (p, lab) in enumerate(batch):\n",
    "        img = Image.open(p).convert('L')  # grayscale\n",
    "        # if augment:\n",
    "        #    # img = augment_pil(img, img_size)\n",
    "        #     # 1. æ°´å¹³ç¿»è½¬\n",
    "        #     if random.random() < 0.5:\n",
    "        #         img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        # \n",
    "        #     # 2. æ—‹è½¬ (-15Â° ~ +15Â°)\n",
    "        #     if random.random() < 0.4:\n",
    "        #         angle = random.uniform(-15, 15)\n",
    "        #         img = img.rotate(angle, resample=Image.BILINEAR, fillcolor=0)\n",
    "        # \n",
    "        #     # 3. ç¼©æ”¾ (0.9x ~ 1.1x)\n",
    "        #     if random.random() < 0.4:\n",
    "        #         scale = random.uniform(0.9, 1.1)\n",
    "        #         w, h = img.size\n",
    "        #         new_w, new_h = int(w * scale), int(h * scale)\n",
    "        #         img = img.resize((new_w, new_h), Image.BILINEAR)\n",
    "        #         # è£å‰ªæˆ–å¡«å……å›åŸå°ºå¯¸ï¼ˆè¿™é‡Œç”¨ center crop + padï¼‰\n",
    "        #         if scale > 1.0:\n",
    "        #             # zoom in: center crop\n",
    "        #             left = (new_w - w) // 2\n",
    "        #             top = (new_h - h) // 2\n",
    "        #             img = img.crop((left, top, left + w, top + h))\n",
    "        #         else:\n",
    "        #             # zoom out: pad with black\n",
    "        #             img = ImageOps.pad(img, (w, h), color=0, centering=(0.5, 0.5))\n",
    "        # \n",
    "        #     # 4. å¹³ç§» (-10% ~ +10% of size)\n",
    "        #     if random.random() < 0.4:\n",
    "        #         w, h = img.size\n",
    "        #         tx = random.uniform(-0.1, 0.1) * w\n",
    "        #         ty = random.uniform(-0.1, 0.1) * h\n",
    "        #         img = img.transform(\n",
    "        #             (w, h),\n",
    "        #             Image.AFFINE,\n",
    "        #             (1, 0, tx, 0, 1, ty),\n",
    "        #             resample=Image.BILINEAR,\n",
    "        #             fillcolor=0\n",
    "        #         )\n",
    "        # \n",
    "        #     # 5. å‰ªåˆ‡ (shear, -10Â° ~ +10Â°)\n",
    "        #     if random.random() < 0.3:\n",
    "        #         shear_angle = random.uniform(-10, 10)  # degrees\n",
    "        #         shear_factor = np.tan(np.radians(shear_angle))\n",
    "        #         w, h = img.size\n",
    "        #         img = img.transform(\n",
    "        #             (w, h),\n",
    "        #             Image.AFFINE,\n",
    "        #             (1, shear_factor, 0, 0, 1, 0),\n",
    "        #             resample=Image.BILINEAR,\n",
    "        #             fillcolor=0\n",
    "        #         )\n",
    "        if augment:\n",
    "            # basic augmentation examples\n",
    "            if random.random() < 0.5:\n",
    "                img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            if random.random() < 0.3:\n",
    "                angle = random.uniform(-10, 10)\n",
    "                img = img.rotate(angle, resample=Image.BILINEAR, fillcolor=0)\n",
    "                \n",
    "        img = img.resize((img_size, img_size), Image.BILINEAR)\n",
    "        X[i] = np.expand_dims(np.asarray(img, dtype=np.float32) / 255.0, axis=0)\n",
    "        y[i] = lab\n",
    "    return X, y\n",
    "\n",
    "def state_dict(model):\n",
    "    \"\"\"\n",
    "    Collect all top-level numpy array attributes from the model.\n",
    "    Returns a dict: {name: np.ndarray}\n",
    "    \"\"\"\n",
    "    sd = {}\n",
    "    for name, val in vars(model).items():\n",
    "        if isinstance(val, np.ndarray):\n",
    "            sd[name] = val\n",
    "    return sd\n",
    "\n",
    "def load_state_dict(model, state, strict=True):\n",
    "    \"\"\"\n",
    "    Load arrays from `state` (a dict of name->ndarray) into model attributes.\n",
    "    If strict=True, raises on missing keys or shape mismatch.\n",
    "    Returns a report dict.\n",
    "    \"\"\"\n",
    "    report = {\"loaded\": [], \"missing_in_model\": [], \"missing_in_state\": [], \"shape_mismatch\": []}\n",
    "    model_vars = vars(model)\n",
    "    # apply\n",
    "    for k, arr in state.items():\n",
    "        if k not in model_vars:\n",
    "            report[\"missing_in_model\"].append(k)\n",
    "            if strict:\n",
    "                raise KeyError(f\"Parameter '{k}' not found in model.\")\n",
    "            continue\n",
    "        if not isinstance(model_vars[k], np.ndarray):\n",
    "            report[\"shape_mismatch\"].append((k, \"not an ndarray on model\"))\n",
    "            if strict:\n",
    "                raise TypeError(f\"Model attribute '{k}' is not an ndarray.\")\n",
    "            continue\n",
    "        if model_vars[k].shape != arr.shape:\n",
    "            report[\"shape_mismatch\"].append((k, (model_vars[k].shape, arr.shape)))\n",
    "            if strict:\n",
    "                raise ValueError(f\"Shape mismatch for '{k}': model {model_vars[k].shape} vs state {arr.shape}\")\n",
    "            # non-strict: still load\n",
    "        setattr(model, k, arr.astype(model_vars[k].dtype, copy=True))\n",
    "        report[\"loaded\"].append(k)\n",
    "    # find params that were expected but not provided\n",
    "    for k, v in model_vars.items():\n",
    "        if isinstance(v, np.ndarray) and k not in state:\n",
    "            report[\"missing_in_state\"].append(k)\n",
    "    return report\n",
    "\n",
    "# def save_model(model, classes, path=\"checkpoints/model_final.npz\", extra_meta=None):\n",
    "#     \"\"\"\n",
    "#     Save model parameters (all top-level ndarrays) and metadata into a .npz file.\n",
    "#     \"\"\"\n",
    "#     os.makedirs(os.path.dirname(path) or \".\", exist_ok=True)\n",
    "#     sd = state_dict(model)\n",
    "#     meta = {\n",
    "#         \"model_class\": model.__class__.__name__,\n",
    "#         \"img_size\": getattr(model, \"img_size\", None),\n",
    "#         \"num_classes\": getattr(model, \"num_classes\", None),\n",
    "#         \"lr\": getattr(model, \"lr\", None),\n",
    "#         \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "#         \"classes\": classes,\n",
    "#     }\n",
    "#     if extra_meta:\n",
    "#         meta.update(extra_meta)\n",
    "#     # pack: all arrays under their own keys + one 'meta' json\n",
    "#     np.savez(path, **sd, meta=np.array([json.dumps(meta)], dtype=object))\n",
    "#     print(f\"ğŸ’¾ Saved {len(sd)} tensors to {path}\")\n",
    "#     return {\"num_tensors\": len(sd), \"path\": path, \"meta\": meta}\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-20T07:36:24.939245Z",
     "start_time": "2025-11-20T07:36:24.930128Z"
    }
   },
   "id": "887dab96f58b915f",
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['bird', 'car', 'cat', 'dog', 'person'] (num_classes=5)\n",
      "\n",
      "=== Epoch 1/30 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 47/47 [00:05<00:00,  8.50it/s, acc=28.6%, loss=1.579]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 Train: loss=1.6726, acc=0.241, time=5.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 16.33it/s, acc=10.0%, loss=1.603]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 Val  : loss=1.5850, acc=0.348, time=1.0s\n",
      "\n",
      "=== Epoch 2/30 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 47/47 [00:05<00:00,  8.50it/s, acc=42.9%, loss=1.520]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02 Train: loss=1.5681, acc=0.297, time=5.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 17.70it/s, acc=20.0%, loss=1.587]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02 Val  : loss=1.5169, acc=0.320, time=0.9s\n",
      "\n",
      "=== Epoch 3/30 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 47/47 [00:05<00:00,  9.21it/s, acc=28.6%, loss=1.514]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03 Train: loss=1.5335, acc=0.336, time=5.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 16.49it/s, acc=10.0%, loss=1.550]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03 Val  : loss=1.4523, acc=0.380, time=1.0s\n",
      "\n",
      "=== Epoch 4/30 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 4:  28%|â–ˆâ–ˆâ–ˆ        | 13/47 [00:01<00:04,  8.34it/s, acc=43.8%, loss=1.475]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[40], line 84\u001B[0m\n\u001B[1;32m     80\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mâœ… Training finished in \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtotal_time\u001B[38;5;241m/\u001B[39m\u001B[38;5;241m60\u001B[39m\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m minutes.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     81\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mğŸ”š Final losses â€” train: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlast_train_loss\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, val: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlast_val_loss\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 84\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[40], line 36\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m()\u001B[0m\n\u001B[1;32m     33\u001B[0m X, y \u001B[38;5;241m=\u001B[39m load_batch(train_items, s, \u001B[38;5;28mmin\u001B[39m(s\u001B[38;5;241m+\u001B[39mbatch_size, \u001B[38;5;28mlen\u001B[39m(train_items)), img_size, augment\u001B[38;5;241m=\u001B[39muse_augment)\n\u001B[1;32m     34\u001B[0m logits,loss \u001B[38;5;241m=\u001B[39m network\u001B[38;5;241m.\u001B[39mloss(X, y)\n\u001B[0;32m---> 36\u001B[0m grads \u001B[38;5;241m=\u001B[39m \u001B[43mnetwork\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgradient\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;66;03m# adam = Adam(lr=lr)\u001B[39;00m\n\u001B[1;32m     38\u001B[0m \u001B[38;5;66;03m# adam.update(network.params, grads = grads)\u001B[39;00m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m network\u001B[38;5;241m.\u001B[39mparams\u001B[38;5;241m.\u001B[39mkeys():\n",
      "Cell \u001B[0;32mIn[37], line 94\u001B[0m, in \u001B[0;36msimpleConvNet.gradient\u001B[0;34m(self, x, t)\u001B[0m\n\u001B[1;32m     92\u001B[0m layers\u001B[38;5;241m.\u001B[39mreverse()\n\u001B[1;32m     93\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m layers:\n\u001B[0;32m---> 94\u001B[0m     dout \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     96\u001B[0m \u001B[38;5;66;03m# è®¾å®š\u001B[39;00m\n\u001B[1;32m     97\u001B[0m grads \u001B[38;5;241m=\u001B[39m {}\n",
      "Cell \u001B[0;32mIn[35], line 42\u001B[0m, in \u001B[0;36mConvolution.backward\u001B[0;34m(self, dout)\u001B[0m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdK \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdK\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39mreshape(FN, C, FH, FW)\n\u001B[1;32m     41\u001B[0m dcol \u001B[38;5;241m=\u001B[39m dout \u001B[38;5;241m@\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcol_K\u001B[38;5;241m.\u001B[39mT\n\u001B[0;32m---> 42\u001B[0m dx \u001B[38;5;241m=\u001B[39m \u001B[43mcol2im\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdcol\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mFH\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mFW\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpad\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     44\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m dx\n",
      "Cell \u001B[0;32mIn[34], line 58\u001B[0m, in \u001B[0;36mcol2im\u001B[0;34m(col, input_shape, filter_h, filter_w, stride, pad)\u001B[0m\n\u001B[1;32m     56\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(filter_w):\n\u001B[1;32m     57\u001B[0m         x_max \u001B[38;5;241m=\u001B[39m x \u001B[38;5;241m+\u001B[39m stride\u001B[38;5;241m*\u001B[39mout_w\n\u001B[0;32m---> 58\u001B[0m         img[:, :, y:y_max:stride, x:x_max:stride] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m col[:, :, y, x, :, :]\n\u001B[1;32m     60\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m img[:, :, pad:H \u001B[38;5;241m+\u001B[39m pad, pad:W \u001B[38;5;241m+\u001B[39m pad]\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def train():\n",
    "    data_root = 'dataset'\n",
    "    epochs = 30\n",
    "    batch_size = 16\n",
    "    img_size = 100\n",
    "    use_augment = True\n",
    "    lr=0.0413\n",
    "    \n",
    "    t0_total = time.time()\n",
    "    \n",
    "    train_items, classes = list_images(os.path.join(data_root, 'train'))\n",
    "    val_items, _ = list_images(os.path.join(data_root, 'val'))\n",
    "    num_classes = len(classes)\n",
    "    print(f'Classes: {classes} (num_classes={num_classes})')\n",
    "    \n",
    "    network = simpleConvNet(input_dim=(1,100,100), \n",
    "                        conv_param = {'filter_nums': [8,16], 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=64, num_classes=5, weight_init_std=0.01)\n",
    "    \n",
    "    last_train_loss, last_val_loss = None, None\n",
    "    \n",
    "    # ---- Prepare save path ----\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        print(f\"\\n=== Epoch {ep}/{epochs} ===\")\n",
    "        random.shuffle(train_items)\n",
    "\n",
    "        # ---- Train ----\n",
    "        t0 = time.time()\n",
    "        loss_accum, acc_accum, seen = 0.0, 0.0, 0\n",
    "        pbar = tqdm(range(0, len(train_items), batch_size), desc=f\"Train {ep}\", ncols=80)\n",
    "        for s in pbar:\n",
    "            X, y = load_batch(train_items, s, min(s+batch_size, len(train_items)), img_size, augment=use_augment)\n",
    "            logits,loss = network.loss(X, y)\n",
    "            \n",
    "            grads = network.gradient(X, y)\n",
    "            # adam = Adam(lr=lr)\n",
    "            # adam.update(network.params, grads = grads)\n",
    "            for key in network.params.keys():\n",
    "                network.params[key] -= lr * grads[key] \n",
    "                \n",
    "            \n",
    "            #loss, dlogits = cross_entropy_loss(probs, y)\n",
    "            #model.backward(dlogits, cache)\n",
    "\n",
    "            acc = accuracy(logits, y)\n",
    "            bsz = X.shape[0]\n",
    "            loss_accum += loss * bsz\n",
    "            acc_accum += acc * bsz\n",
    "            seen += bsz\n",
    "            pbar.set_postfix(loss=f\"{loss:.3f}\", acc=f\"{acc*100:.1f}%\")\n",
    "\n",
    "        last_train_loss = loss_accum / max(1, seen)\n",
    "        train_time = time.time() - t0\n",
    "        print(f\"Epoch {ep:02d} Train: loss={last_train_loss:.4f}, acc={acc_accum/seen:.3f}, time={train_time:.1f}s\")\n",
    "\n",
    "        # ---- Val ----\n",
    "        t1 = time.time()\n",
    "        val_loss, val_acc, vseen = 0.0, 0.0, 0\n",
    "        pbar_val = tqdm(range(0, len(val_items), batch_size), desc=f\"Val {ep}\", ncols=80)\n",
    "        for s in pbar_val:\n",
    "            Xv, yv = load_batch(val_items, s, min(s+batch_size, len(val_items)), img_size, augment=False)\n",
    "            # logits, _ = model.forward(Xv)\n",
    "            # probs = softmax(logits)\n",
    "            # l, _ = cross_entropy_loss(probs, yv)\n",
    "            \n",
    "            logits, l = network.loss(Xv, yv)\n",
    "            a = accuracy(logits, yv)\n",
    "            bsz = Xv.shape[0]\n",
    "            val_loss += l * bsz\n",
    "            val_acc += a * bsz\n",
    "            vseen += bsz\n",
    "            pbar_val.set_postfix(loss=f\"{l:.3f}\", acc=f\"{a*100:.1f}%\")\n",
    "\n",
    "        last_val_loss = val_loss / max(1, vseen)\n",
    "        val_time = time.time() - t1\n",
    "        print(f\"Epoch {ep:02d} Val  : loss={last_val_loss:.4f}, acc={val_acc/vseen:.3f}, time={val_time:.1f}s\")\n",
    "\n",
    "    total_time = time.time() - t0_total\n",
    "    print(f\"\\nâœ… Training finished in {total_time/60:.2f} minutes.\")\n",
    "    print(f\"ğŸ”š Final losses â€” train: {last_train_loss:.4f}, val: {last_val_loss:.4f}\")\n",
    "    \n",
    "\n",
    "train()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-20T07:36:45.586444Z",
     "start_time": "2025-11-20T07:36:24.939954Z"
    }
   },
   "id": "7c47a00046c04b11",
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    " "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-20T07:36:45.588205Z",
     "start_time": "2025-11-20T07:36:45.587738Z"
    }
   },
   "id": "2c2be80d7e323cf",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
