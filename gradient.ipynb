{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 梯度下降法\n",
    "1、梯度 是由 函数的全部变量的偏导数汇总而成的向量\n",
    "例如：$f(x_{1},x_{2}) = x_{1}^{2} + x_{2}^{2}$\n",
    "那么 在点（3，4） f(x1,x2)的梯度是：$f^{'}(x_{1},x_{2}) = (2x_{1},2x_{2}) = (6,8)$\n",
    "这个向量（6，8） 就是函数在这个点的梯度"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "624ef7f059f12364"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-01T07:44:19.978706Z",
     "start_time": "2025-11-01T07:44:19.976495Z"
    }
   },
   "id": "b674848bf921f76c",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    return np.sum(x**2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-01T07:55:23.340434Z",
     "start_time": "2025-11-01T07:55:23.334536Z"
    }
   },
   "id": "e4f9c0e1ebebd7ae",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "## 求 函数f 在 x点处的梯度数值解\n",
    "# def numerical_gradient(f,x):\n",
    "#     h = 1e-4\n",
    "#     grad = np.zeros_like(x)\n",
    "#     # 求x1的偏导数的时候，x2的值直接带入。求x2的偏导数，x1的值直接带入\n",
    "#     for idx in range(x.size):\n",
    "#         tmp_val = x[idx]\n",
    "#         \n",
    "#         x[idx] = tmp_val + h\n",
    "#         fxh1 = f(x)\n",
    "#         \n",
    "#         x[idx] = tmp_val - h\n",
    "#         fxh2 = f(x)\n",
    "#         \n",
    "#         grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "#         x[idx] = tmp_val\n",
    "#     \n",
    "#     return grad\n",
    "\n",
    "def numerical_gradient(f, W, eps=1e-5):\n",
    "    grad = np.zeros_like(W)\n",
    "    it = np.nditer(W, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        orig = W[idx]\n",
    "        \n",
    "        W[idx] = orig + eps\n",
    "        loss_plus = f(W)\n",
    "        \n",
    "        W[idx] = orig - eps\n",
    "        loss_minus = f(W)\n",
    "        \n",
    "        W[idx] = orig\n",
    "        \n",
    "        grad[idx] = (loss_plus - loss_minus) / (2 * eps)\n",
    "        it.iternext()  # 移动到下一个元素\n",
    "    return grad"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-01T09:17:45.891367Z",
     "start_time": "2025-11-01T09:17:45.886447Z"
    }
   },
   "id": "b7ed5eba0474dcb9",
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-6.11110793e-10  8.14814392e-10]\n"
     ]
    }
   ],
   "source": [
    "def gradient_decent(f, init_x, lr=0.01, step_num = 100):\n",
    "    x = init_x\n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr*grad\n",
    "        \n",
    "    return x\n",
    "\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "print(gradient_decent(function_2, init_x=init_x, lr=0.1))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-01T09:17:47.988286Z",
     "start_time": "2025-11-01T09:17:47.980188Z"
    }
   },
   "id": "cca27da6fc432481",
   "execution_count": 44
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 神经网络的梯度\n",
    "神经网络的梯度是：损失函数关于权重参数的梯度"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f67d565d4b130c1e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    max_value = np.max(z)\n",
    "    exp_z = np.exp(z - max_value)\n",
    "    sum_max_z = np.sum(exp_z)\n",
    "    y = exp_z/sum_max_z\n",
    "    return y\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7\n",
    "    if y.ndim ==1:\n",
    "        y = y.reshape(1, y.size)\n",
    "        t = t.reshape(1, t.size)\n",
    "        \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t*np.log(y+delta))/batch_size"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-01T08:22:59.151253Z",
     "start_time": "2025-11-01T08:22:59.145366Z"
    }
   },
   "id": "952b5c75a1372a46",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.09958317 -2.37724888  0.12666433]\n",
      " [-0.5149041   0.91928847  0.21565623]]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "class simple_net:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return x@self.W\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "        return loss\n",
    "\n",
    "net = simple_net()\n",
    "print(net.W)\n",
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "print(np.argmax(p))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-01T09:18:25.817079Z",
     "start_time": "2025-11-01T09:18:25.810882Z"
    }
   },
   "id": "3fc9cb051a78c0f",
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "t = np.array([0, 1, 0])\n",
    "#print(net.loss(x, t))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-01T09:18:27.866430Z",
     "start_time": "2025-11-01T09:18:27.862794Z"
    }
   },
   "id": "161e50d28b37a2f3",
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def f(W):\n",
    "    return net.loss(x, t)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-01T09:19:13.894134Z",
     "start_time": "2025-11-01T09:19:13.891499Z"
    }
   },
   "id": "8fe735ac93a0f921",
   "execution_count": 51
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 解释\n",
    "怎么理解 数值方法求w关于损失函数的梯度：\n",
    "对于W里面的每个值，都变化一点 相加减 eps=1-e5后，求新的损失函数的值 loss_plus loss_minus。然后通过公式(loss_plus - loss_minus) / (2 * eps) 得到对应的w元素变化后它的梯度值是多少"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e11c7dd77154a862"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.1450132  -0.46557578  0.32056259]\n",
      " [ 0.21751979 -0.69836367  0.48084388]]\n"
     ]
    }
   ],
   "source": [
    "dw = numerical_gradient(f, net.W)\n",
    "print(dw)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-01T09:18:39.957680Z",
     "start_time": "2025-11-01T09:18:39.953225Z"
    }
   },
   "id": "44662b00b7a59e92",
   "execution_count": 50
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "81b510a015e3af08"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
